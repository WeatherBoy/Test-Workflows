\section{Methodology}
\subsection{Reaching out to the industry}
While writing my thesis, I hoped that the software solutions I was developing could build the foundation for a start-up. However, multiple complications arose. I ended up working on three different projects, and even the "easiest", at least from a software development point of view, quickly hit a wall. It ultimately proved too difficult to implement, simply because the data sources were too inaccessible. While the two other projects might have worked in theory, they would still be riddled with infant diseases as soon as they reached production, due to the data inaccessibility issues. Seeing as the healthcare sector is already full of disjoint software solutions, usually designed with clunky or, at best, simply odd workarounds, I elected to refrain from contributing my own disjoint software solution and thus abandoned the start-up dream.
\\
Yet, it remained clear to me that this underlying issue of data inaccessibility persisted. Therefore, I became quite hopeful upon reading the 2024 report by "Danske Regioner" ~\cite{Den-Reg-digitalisation}. In the report, "Danske Regioner" proposed a so-called "Danish Healthcare Cloud". I have since come to think of the \textbf{Healthcare Cloud} as a pseudo-appstore; however, it might be more aptly described as a combination of a data warehouse, or data lake, and an appstore. The core functionality of the \textbf{Healthcare Cloud} would be a platform which allowed for easy data sharing. In other words, all applications on the \textbf{Healthcare Cloud} would be required to store their data in the \textbf{Healthcare Cloud}, such that any other application on the \textbf{Healthcare Cloud} would have the same data readily accessible. This would immediately eliminate any need for the healthcare personnel to perform time-wasting double (triple or even more) loggings of the data. However, the \textbf{Healthcare Cloud} would also need to support a universal login for the healthcare personnel, such that they don't have to keep separate log-ins for each platform. Additionally, the \textbf{Healthcare Cloud} would have to handle some kind of licensing to verify that it is the right developers who can create platforms with access to the data.
\\
Upon finishing my thesis in March of 2025, I sought to initiate contact with "Danske Regioner" in the hopes that I might be able to contribute to the project. Unfortunately, "Danske Regioner" isn't, as of writing, currently working on that project. Instead, they had simply collaborated with a tech consultancy to outline the \textbf{Healthcare Cloud}, what problems it could solve and how it could be developed. "Danske Regioner" responded kindly to my message and advised that I should reach out to CIMT ("Center for IT \& Medico-Teknologi") in the capital region. They deemed that CIMT might be able to allocate resources for such a project.
\\
It took roughly two months before I got to have a meeting with CIMT. They were two very nice and formal representatives who, in some capacity, worked with planning and scheduling data within the healthcare sector of the capital region. However, although they also found the project of interest, they very quickly shut it down. They informed that CIMT had neither the resources nor the authority to take on a project such as the \textbf{Healthcare Cloud}.
\\
Another big aspect of realising the \textbf{Healthcare Cloud} is data standardisation. Unless the healthcare institutions can somehow come to an agreement on how to label all data points, the \textbf{Healthcare Cloud} would still not allow for cross-institutional data sharing. So even if CIMT managed to standardise the healthcare data within the capital region, it would remain incompatible with the data from the other four regions\footnote{The five regions being: The Capital Region, The Region of Northern Jutland, The Region of Central Jutland, The Region of Southern Denmark, and The Region of Zealand}. As such, CIMT ultimately rerouted me back to "Danske Regioner", as, if anybody had to "make the decree", so to speak, of data standardisation, it would be them.
\\
Upon reaching out to "Danske Regioner" once more, I was, to my great joy, met by a very kind secretary, who remembered my original call. She seemed impressed that I had managed to secure a meeting with CIMT, and she promptly directed me further up the administrative chain. Luckily, this time it was simply a phone call, and it didn't take two months to plan. However, the administrative worker I got a hold of at "Danske Regioner" stated that, although it had been the staff at CIMT's impression, "Danske Regioner" doesn't handle administrative software on that kind of scale. Instead, I was redirected to the Danish Health Data Authority (DHDA).
\\
I was fully aware that DHDA wasn't working on a \textbf{Healthcare Cloud}, so when I reached out to them, it was more so in the hopes of creating a short collaboration for a medium-sized project at DTU. Maybe they could provide some actual or mock data from the Danish healthcare sector, and hopefully, I could contribute something useful with that. Unfortunately, that wasn't the case. DHDA found my motivation endearing and recommended that I apply for a job at their department, as they hoped there would come an opening where I would have the possibility of working on something adjacent to the \textbf{Healthcare Cloud}. Nevertheless, DHDA proved a dead end all the same, and as such, I was unable to secure data for this project from an official source.

\subsection{Proceeding without clinical data}
\subsubsection*{Initial idea: The Healthcare Cloud prototype}
Without direct access to clinical data, I originally had the thought of trying to set up a prototype for a platform with the same capabilities as the \textbf{Healthcare Cloud} as proposed by "Danske Regioner".
\\
To me, it seemed like Rust might have been a great language for a platform which required such great detail and intricate work. However, I also researched the possibilities of using languages such as either Go or Python. Furthermore, I tried to figure out what would serve as a good frontend, whether it be pure CSS and HTML or maybe a combination of SvelteKit with Rust. Unfortunately, it quickly became clear that starting off from a near-naked foundation wouldn't bring about any results within a reasonable timeframe for a 10 ECTS course. As such, I tried to set up a backend of C\# with ASP.NET Core in Visual Studio 2022 with a frontend of ASP.NET Core Web APP, seeing as it would provide a more enterprise-mature framework for a project the size of the \textbf{Healthcare Cloud}. It had many advantages, such as allowing for easy connection with Microsoft Azure, which already has a mature development environment for healthcare applications within the EU regions, which in turn could make developing around FHIR easier. Ultimately, the task proved too big and intangible for a one-man operation. It was simply too difficult to clearly define an achievable goal. Additionally, my supervisor and I were interested in the possibilities of testing the UX capabilities of the product. This would have been nearly impossible with a platform intended to service third-party applications, considering the infinitesimal likelihood of 1) me developing such a platform on my lonesome \textbf{and} 2) some third-party developer being willing to migrate to that platform to test the user experience. Consequently, although quite a bit of thought and research had been poured into it, the early \textbf{Healthcare Cloud} prototype approach was abandoned.

\subsubsection*{Second idea: defining a common care pathway}
Still coming up empty-handed with regards to official clinical data, my supervisor proposed a hypothesis: "Is it possible to define a common care pathway for multiple patients that broadly requires the same data for their GPs?".
\\
An example of what a common care pathway might look like can be seen in \autoref{fig:common-care-pathway}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{figures/methodology/common_care_pathway.png}
    \caption{Common care pathway example}
    \medskip
    \small
    \raggedright
    An example of a common care pathway that could be defined for multiple patients. Should be read from left to right following the arrows.
    \label{fig:common-care-pathway}
\end{figure}

\noindent
The idea was that if such a common care pathway could be defined, then it would be possible, from a technical point of view, to gather the data a GP would like to have available at any given point during the care pathway. With the data in hand, it could be presented as either an infographic or a report with the intent of giving the GP a better overview, without having to seek out this data themselves from possibly multiple different sources. If the data were chosen expertly and adequately for the common care pathway, and presented in a clear fashion, the GP might not even have to seek out this data themselves or, hopefully, in the worst case, they would only have to look up a few missing details. Thus, if a common care pathway could be found, I would be able to showcase how such data could be retrieved using FHIR and test out the UX for a report/ infographic. Therefore, I promptly consulted a few of my friends who are in the process of studying to become medical practitioners themselves.

\subsubsection*{Pregnant women and diabetes patients}
From discussing the proposition with my friends, I gathered a lot. For one, I got a deeper understanding of a Danish medical patient journal platform called "Sundhedsplatformen" (SP). I also learned about polypharmacy, both of which I'll return to later. However, quickly, it became crystal clear that in order to simply pinpoint a common care pathway, not even considering defining what data a GP would require throughout, necessitated a lot more expertise and practice than my friends, who, and, as of writing, are "only" students, possessed. Here, pregnant women and diabetes patients were shown to be a blessing in disguise (for the purposes of this project at least).
\\
A common denominator between diabetes patients and pregnant women is that their "care pathways", or, maybe more so, consultations are pretty set in stone. For pregnant women, we know almost to a tee how their progress is gonna be. For diabetes patients, the overall structure of their consultations is fairly consistent. It usually takes the shape of a bi-annual "check-up". Which doesn't differ much in content every six months.
\\
We had struck gold! In collaboration with my friends, I had found not only a single common care pathway, but two very strong candidates. Yet, a fundamental problem persisted, Expertise.
\\
Although we could find plenty of information \textbf{for} pregnant women and how their progress is supposed to look, even down to the week, it was arduous to figure out what kind of data the GP would be interested in. What kind of questions would the GP be asking, and so on and so forth. 
\\
It was a similar problem for diabetes, because although my friends had had courses pertaining to diabetes patients and the disease itself, they lacked the expertise of actual medical practitioners who have sat face-to-face with a diabetes patient in consultation. They could point out common sequelae of diabetes and what to be cautious of in textbooks, but it still seemed inadequate for defining strict criteria of required data, adhering to consultation with a diabetes patient. 
\\
One of my friends even posted for me in a group consisting of alumni and currently practitioning doctors, as well as students with more clinical experience, with a message where I asked if anybody would be up for roughly an hour of interview for a student project. Regretably, but also understandably, considering their busy schedules, nobody wrote back.
\\
It was at this point that my supervisor proposed I use the same questionnaire instruments as the DiaFocus project.


\subsection{Generating reports for medical consultations based on questionnaire responses}
Now the path was laid. I would use the same eight questionnaires as detailed in the DiaFocus project and as described in \autoref{sec:theory-questionnaire}. I started by gathering some qualitative data by spending roughly an hour interviewing a type-1 diabetes patient. We went through all eight questionnaires, a process I will elaborate on further in the analysis. The questionnaire answers from this interview can be seen in appendix \autoref{sec:questionnaire-answers}. With the answers in hand, I set out to convert them into an actual HTML report.
\\
I did this by combining a JSON file of responses with matching JSON files of the questionnaire instruments. The JSON file of responses can be found in a directory named after the patient's ID. As I only had one response, that response can be found in the "P001" directory. The responses were titled according to the date of the response in accordance with the ISO 8601 standard~\cite{Wiki-ISO-8601}, or perhaps more familiarly, as [YYYY]-[MM]-[DD]. This allowed for the program to, by default, present the newest responses as ISO 8601 is in lexicographic order, from newest to oldest.
\\
The responses took the following form as JSON files:

\begin{minted}{json}
{
    "patient_id": "P-XXX",
    "visit_date": "YYYY-MM-DD",
    "questionnaires": {
        "questionnaire_id": {
            "question_id": "answer",
            "question_id": {
                "answer": "answer",
                "comment": "comment"
            }
            "...": "..."
        },
        "...": "..."
    }
}
\end{minted}

\noindent
From the above JSON snippet, we can see that for the \texttt{"question_id"}, its values can take two different forms. Either it can be a plain answer, as indicated by the \texttt{"answer"} value, or it can be a tuple of both \texttt{"answer"} and \texttt{"comment"}. This was introduced with the intended purpose that the user, the diabetes patient, could add additional information when filling out the questionnaire, if they felt it was important for the GP to take note. This, however, introduced some complications to the system as the \emph{comments} had to be filtered out when scoring and labelling the answers, but stored, so that they could later be reintroduced into the final HTML report.
\\
\\
The instrument JSON files were placed in a separate directory labelled "instruments". They were simply titled with a shorthand for the appropriate instrument. The approach here wasn't extremely sophisticated, as the program simply always goes through all the JSON files in the "instruments" directory and checks whether there is a corresponding response in the response JSON.
\\
The instruments then took the following form as JSON files:
\begin{minted}{json}
{
    "instrument_id": "questionnaire_id",
    "title": "questionnaire title",
    "version": "X.X",
    "scales": {
        "scale": {
            "scale_id": "id",
            "range": ["min", "max"],
            "labels": {
                "value": "label",
                "...": "..."
            }
        },
        "...": "..."
    },
    "validation": {
        "default_required": "boolean",
        "optional_questions": ["question_id", "..."]
    },
    "questions": [
        {
            "id": "question_id",
            "text": "question text",
            "type": "text/number/choice/...",
            "scale": "scale_id",
        },
        "...": "..."
    ]
}
\end{minted}

\noindent
For the way I set up the processing, it was of utmost importance that the \texttt{"instrument_id"} of the instrument JSON matched the \texttt{"questionnaire_id"} of the response JSON. Likewise for the \texttt{"id"} tag's value (\texttt{"question_id"}) in the \texttt{"questions"} list of the instrument JSON and the \texttt{"question_id"} tag in the response JSON.
\\
\\
In \autoref{fig:python-system-overview} is an attempt at giving an (incomplete) overview of how the system works. The system is built entirely in Python, with the exception of a bit of HTML code, in the form of HTML templates used in conjunction with the \texttt{Jinja2} engine. The codebase can be seen in its entirety on GitHub \github{https://github.com/WeatherBoy/Test-Workflows}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\textwidth]{figures/methodology/python_system.png}
    \caption{Overview of the Python system architecture}
    \medskip
    \small
    \raggedright
    An overview of the Python system architecture. The processed \texttt{responses} and \texttt{instruments} are then merged into a final HTML report.
    \label{fig:python-system-overview}
\end{figure}

\noindent
After a lot of work and frustrating debugging (as it often is with coding), I ended up with an HTML report ready for testing. A PDF version of the report can be found in appendix \autoref{sec:html-report-pdf}.
